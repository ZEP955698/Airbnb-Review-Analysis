{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "import gensim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert csv to feather \n",
    "airbnb_reviews = pd.read_csv('data/airbnb_reviews.csv', delimiter=';', skiprows=0, low_memory=False).dropna()\n",
    "airbnb_reviews.reset_index().to_feather('data/airbnb_reviews.feather')\n",
    "\n",
    "air_bnb_ret_new = pd.read_csv('data/airbnb_ratings_new.csv',engine = 'python').dropna()\n",
    "air_bnb_ret_new.reset_index().to_feather('data/air_bnb_ret_new.feather')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>488835</td>\n",
       "      <td>104522929</td>\n",
       "      <td>2016-09-27</td>\n",
       "      <td>66003975</td>\n",
       "      <td>Carole</td>\n",
       "      <td>First time using Airbnb and couldn't be happie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>549036</td>\n",
       "      <td>37751194</td>\n",
       "      <td>2015-07-10</td>\n",
       "      <td>33501858</td>\n",
       "      <td>Gabi</td>\n",
       "      <td>Muy bien ubicado. Las fotos y la descrpción co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id         id        date  reviewer_id reviewer_name  \\\n",
       "0      488835  104522929  2016-09-27     66003975        Carole   \n",
       "1      549036   37751194  2015-07-10     33501858          Gabi   \n",
       "\n",
       "                                            comments  \n",
       "0  First time using Airbnb and couldn't be happie...  \n",
       "1  Muy bien ubicado. Las fotos y la descrpción co...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_reviews = pd.read_feather('data/airbnb_reviews.feather').drop('index', axis=1)\n",
    "airbnb_reviews.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Listing ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Host ID</th>\n",
       "      <th>Host Name</th>\n",
       "      <th>Host Response Rate</th>\n",
       "      <th>Host Is Superhost</th>\n",
       "      <th>Host total listings count</th>\n",
       "      <th>Street</th>\n",
       "      <th>City</th>\n",
       "      <th>Neighbourhood cleansed</th>\n",
       "      <th>...</th>\n",
       "      <th>Number of reviews</th>\n",
       "      <th>Last Review Date</th>\n",
       "      <th>Review Scores Rating</th>\n",
       "      <th>Review Scores Accuracy</th>\n",
       "      <th>Review Scores Cleanliness</th>\n",
       "      <th>Review Scores Checkin</th>\n",
       "      <th>Review Scores Communication</th>\n",
       "      <th>Review Scores Location</th>\n",
       "      <th>Review Scores Value</th>\n",
       "      <th>Reviews per month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5534229.0</td>\n",
       "      <td>A 2 Passi da San Pietro</td>\n",
       "      <td>28697142.0</td>\n",
       "      <td>Veronica</td>\n",
       "      <td>100%</td>\n",
       "      <td>False</td>\n",
       "      <td>5.0</td>\n",
       "      <td>00165| Rm 00165| Italy</td>\n",
       "      <td>165</td>\n",
       "      <td>XIII Aurelia</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8/29/15</td>\n",
       "      <td>90</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5903406.0</td>\n",
       "      <td>cosy small apartment</td>\n",
       "      <td>1853799.0</td>\n",
       "      <td>Veronika</td>\n",
       "      <td>88%</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1190| Wien| Austria</td>\n",
       "      <td>1190</td>\n",
       "      <td>D�bling</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9/9/17</td>\n",
       "      <td>87</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Listing ID                     Name     Host ID Host Name  \\\n",
       "0   5534229.0  A 2 Passi da San Pietro  28697142.0  Veronica   \n",
       "1   5903406.0     cosy small apartment   1853799.0  Veronika   \n",
       "\n",
       "  Host Response Rate  Host Is Superhost  Host total listings count  \\\n",
       "0               100%              False                        5.0   \n",
       "1                88%              False                        2.0   \n",
       "\n",
       "                   Street  City Neighbourhood cleansed  ... Number of reviews  \\\n",
       "0  00165| Rm 00165| Italy   165           XIII Aurelia  ...               2.0   \n",
       "1     1190| Wien| Austria  1190                D�bling  ...               3.0   \n",
       "\n",
       "  Last Review Date Review Scores Rating  Review Scores Accuracy  \\\n",
       "0          8/29/15                   90                     9.0   \n",
       "1           9/9/17                   87                     9.0   \n",
       "\n",
       "  Review Scores Cleanliness Review Scores Checkin  \\\n",
       "0                      10.0                   8.0   \n",
       "1                      10.0                  10.0   \n",
       "\n",
       "   Review Scores Communication  Review Scores Location  Review Scores Value  \\\n",
       "0                          8.0                     9.0                  9.0   \n",
       "1                         10.0                    10.0                  8.0   \n",
       "\n",
       "  Reviews per month  \n",
       "0              0.08  \n",
       "1              0.27  \n",
       "\n",
       "[2 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_bnb_ret_new = pd.read_feather('data/air_bnb_ret_new.feather').drop('index', axis=1)\n",
    "air_bnb_ret_new.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select rows with only English comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0000 data have been processed\n",
      "2 0000 data have been processed\n",
      "3 0000 data have been processed\n",
      "4 0000 data have been processed\n",
      "5 0000 data have been processed\n",
      "6 0000 data have been processed\n",
      "7 0000 data have been processed\n",
      "8 0000 data have been processed\n",
      "9 0000 data have been processed\n",
      "10 0000 data have been processed\n",
      "11 0000 data have been processed\n",
      "12 0000 data have been processed\n",
      "13 0000 data have been processed\n",
      "14 0000 data have been processed\n",
      "15 0000 data have been processed\n",
      "16 0000 data have been processed\n",
      "17 0000 data have been processed\n",
      "18 0000 data have been processed\n",
      "19 0000 data have been processed\n",
      "20 0000 data have been processed\n",
      "21 0000 data have been processed\n",
      "22 0000 data have been processed\n",
      "23 0000 data have been processed\n",
      "24 0000 data have been processed\n",
      "25 0000 data have been processed\n",
      "26 0000 data have been processed\n",
      "27 0000 data have been processed\n",
      "28 0000 data have been processed\n",
      "29 0000 data have been processed\n",
      "30 0000 data have been processed\n",
      "31 0000 data have been processed\n",
      "32 0000 data have been processed\n",
      "33 0000 data have been processed\n",
      "34 0000 data have been processed\n",
      "35 0000 data have been processed\n",
      "36 0000 data have been processed\n",
      "37 0000 data have been processed\n",
      "38 0000 data have been processed\n",
      "39 0000 data have been processed\n",
      "40 0000 data have been processed\n",
      "41 0000 data have been processed\n",
      "42 0000 data have been processed\n",
      "43 0000 data have been processed\n",
      "44 0000 data have been processed\n",
      "45 0000 data have been processed\n",
      "46 0000 data have been processed\n",
      "47 0000 data have been processed\n",
      "48 0000 data have been processed\n",
      "49 0000 data have been processed\n",
      "50 0000 data have been processed\n",
      "51 0000 data have been processed\n",
      "52 0000 data have been processed\n",
      "53 0000 data have been processed\n",
      "54 0000 data have been processed\n",
      "55 0000 data have been processed\n",
      "56 0000 data have been processed\n",
      "57 0000 data have been processed\n",
      "58 0000 data have been processed\n",
      "59 0000 data have been processed\n",
      "60 0000 data have been processed\n",
      "61 0000 data have been processed\n",
      "62 0000 data have been processed\n",
      "63 0000 data have been processed\n",
      "64 0000 data have been processed\n",
      "65 0000 data have been processed\n",
      "66 0000 data have been processed\n",
      "67 0000 data have been processed\n",
      "68 0000 data have been processed\n",
      "69 0000 data have been processed\n",
      "70 0000 data have been processed\n",
      "71 0000 data have been processed\n",
      "72 0000 data have been processed\n",
      "73 0000 data have been processed\n",
      "74 0000 data have been processed\n",
      "75 0000 data have been processed\n",
      "76 0000 data have been processed\n",
      "77 0000 data have been processed\n",
      "78 0000 data have been processed\n",
      "79 0000 data have been processed\n",
      "80 0000 data have been processed\n",
      "81 0000 data have been processed\n",
      "82 0000 data have been processed\n",
      "83 0000 data have been processed\n",
      "84 0000 data have been processed\n",
      "85 0000 data have been processed\n",
      "86 0000 data have been processed\n",
      "87 0000 data have been processed\n",
      "88 0000 data have been processed\n",
      "89 0000 data have been processed\n",
      "90 0000 data have been processed\n",
      "91 0000 data have been processed\n",
      "92 0000 data have been processed\n",
      "93 0000 data have been processed\n",
      "94 0000 data have been processed\n",
      "95 0000 data have been processed\n",
      "96 0000 data have been processed\n",
      "97 0000 data have been processed\n",
      "98 0000 data have been processed\n",
      "99 0000 data have been processed\n",
      "100 0000 data have been processed\n",
      "101 0000 data have been processed\n",
      "102 0000 data have been processed\n",
      "103 0000 data have been processed\n",
      "104 0000 data have been processed\n",
      "105 0000 data have been processed\n",
      "106 0000 data have been processed\n",
      "107 0000 data have been processed\n",
      "108 0000 data have been processed\n",
      "109 0000 data have been processed\n",
      "110 0000 data have been processed\n",
      "111 0000 data have been processed\n",
      "112 0000 data have been processed\n",
      "113 0000 data have been processed\n",
      "114 0000 data have been processed\n",
      "115 0000 data have been processed\n",
      "116 0000 data have been processed\n",
      "117 0000 data have been processed\n",
      "118 0000 data have been processed\n",
      "119 0000 data have been processed\n",
      "120 0000 data have been processed\n",
      "121 0000 data have been processed\n",
      "122 0000 data have been processed\n",
      "123 0000 data have been processed\n",
      "124 0000 data have been processed\n",
      "125 0000 data have been processed\n",
      "126 0000 data have been processed\n",
      "127 0000 data have been processed\n",
      "128 0000 data have been processed\n",
      "129 0000 data have been processed\n",
      "130 0000 data have been processed\n",
      "131 0000 data have been processed\n",
      "132 0000 data have been processed\n",
      "133 0000 data have been processed\n",
      "134 0000 data have been processed\n",
      "135 0000 data have been processed\n",
      "136 0000 data have been processed\n",
      "137 0000 data have been processed\n",
      "138 0000 data have been processed\n",
      "139 0000 data have been processed\n",
      "140 0000 data have been processed\n",
      "141 0000 data have been processed\n",
      "142 0000 data have been processed\n",
      "143 0000 data have been processed\n",
      "144 0000 data have been processed\n",
      "145 0000 data have been processed\n",
      "146 0000 data have been processed\n",
      "147 0000 data have been processed\n",
      "148 0000 data have been processed\n",
      "149 0000 data have been processed\n",
      "150 0000 data have been processed\n",
      "151 0000 data have been processed\n",
      "152 0000 data have been processed\n",
      "153 0000 data have been processed\n",
      "154 0000 data have been processed\n",
      "155 0000 data have been processed\n",
      "156 0000 data have been processed\n",
      "157 0000 data have been processed\n",
      "158 0000 data have been processed\n",
      "159 0000 data have been processed\n",
      "160 0000 data have been processed\n",
      "161 0000 data have been processed\n",
      "162 0000 data have been processed\n",
      "163 0000 data have been processed\n",
      "164 0000 data have been processed\n",
      "165 0000 data have been processed\n",
      "166 0000 data have been processed\n",
      "167 0000 data have been processed\n",
      "168 0000 data have been processed\n",
      "169 0000 data have been processed\n",
      "170 0000 data have been processed\n",
      "171 0000 data have been processed\n",
      "172 0000 data have been processed\n",
      "173 0000 data have been processed\n",
      "174 0000 data have been processed\n",
      "175 0000 data have been processed\n",
      "176 0000 data have been processed\n",
      "177 0000 data have been processed\n",
      "178 0000 data have been processed\n",
      "179 0000 data have been processed\n",
      "180 0000 data have been processed\n",
      "181 0000 data have been processed\n",
      "182 0000 data have been processed\n",
      "183 0000 data have been processed\n",
      "184 0000 data have been processed\n",
      "185 0000 data have been processed\n",
      "186 0000 data have been processed\n",
      "187 0000 data have been processed\n",
      "188 0000 data have been processed\n",
      "189 0000 data have been processed\n",
      "190 0000 data have been processed\n",
      "191 0000 data have been processed\n",
      "192 0000 data have been processed\n",
      "193 0000 data have been processed\n",
      "194 0000 data have been processed\n",
      "195 0000 data have been processed\n",
      "196 0000 data have been processed\n",
      "197 0000 data have been processed\n",
      "198 0000 data have been processed\n",
      "199 0000 data have been processed\n",
      "200 0000 data have been processed\n",
      "201 0000 data have been processed\n",
      "202 0000 data have been processed\n",
      "203 0000 data have been processed\n",
      "204 0000 data have been processed\n",
      "205 0000 data have been processed\n",
      "206 0000 data have been processed\n",
      "207 0000 data have been processed\n",
      "208 0000 data have been processed\n",
      "209 0000 data have been processed\n",
      "210 0000 data have been processed\n",
      "211 0000 data have been processed\n",
      "212 0000 data have been processed\n",
      "213 0000 data have been processed\n",
      "214 0000 data have been processed\n",
      "215 0000 data have been processed\n",
      "216 0000 data have been processed\n",
      "217 0000 data have been processed\n",
      "218 0000 data have been processed\n",
      "219 0000 data have been processed\n",
      "220 0000 data have been processed\n",
      "221 0000 data have been processed\n",
      "222 0000 data have been processed\n",
      "223 0000 data have been processed\n",
      "224 0000 data have been processed\n",
      "225 0000 data have been processed\n",
      "226 0000 data have been processed\n",
      "227 0000 data have been processed\n",
      "228 0000 data have been processed\n",
      "229 0000 data have been processed\n",
      "230 0000 data have been processed\n",
      "231 0000 data have been processed\n",
      "232 0000 data have been processed\n",
      "233 0000 data have been processed\n",
      "234 0000 data have been processed\n",
      "235 0000 data have been processed\n",
      "236 0000 data have been processed\n",
      "237 0000 data have been processed\n",
      "238 0000 data have been processed\n",
      "239 0000 data have been processed\n",
      "240 0000 data have been processed\n",
      "241 0000 data have been processed\n",
      "242 0000 data have been processed\n",
      "243 0000 data have been processed\n",
      "244 0000 data have been processed\n",
      "245 0000 data have been processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246 0000 data have been processed\n",
      "247 0000 data have been processed\n",
      "248 0000 data have been processed\n",
      "249 0000 data have been processed\n",
      "250 0000 data have been processed\n",
      "251 0000 data have been processed\n",
      "252 0000 data have been processed\n",
      "253 0000 data have been processed\n",
      "254 0000 data have been processed\n",
      "255 0000 data have been processed\n",
      "256 0000 data have been processed\n",
      "257 0000 data have been processed\n",
      "258 0000 data have been processed\n",
      "259 0000 data have been processed\n",
      "260 0000 data have been processed\n",
      "261 0000 data have been processed\n",
      "262 0000 data have been processed\n",
      "263 0000 data have been processed\n",
      "264 0000 data have been processed\n",
      "265 0000 data have been processed\n",
      "266 0000 data have been processed\n",
      "267 0000 data have been processed\n",
      "268 0000 data have been processed\n",
      "269 0000 data have been processed\n",
      "270 0000 data have been processed\n",
      "271 0000 data have been processed\n",
      "272 0000 data have been processed\n",
      "273 0000 data have been processed\n",
      "274 0000 data have been processed\n",
      "275 0000 data have been processed\n",
      "276 0000 data have been processed\n",
      "277 0000 data have been processed\n",
      "278 0000 data have been processed\n",
      "279 0000 data have been processed\n",
      "280 0000 data have been processed\n",
      "281 0000 data have been processed\n",
      "282 0000 data have been processed\n",
      "283 0000 data have been processed\n",
      "284 0000 data have been processed\n",
      "285 0000 data have been processed\n",
      "286 0000 data have been processed\n",
      "287 0000 data have been processed\n",
      "288 0000 data have been processed\n",
      "289 0000 data have been processed\n",
      "290 0000 data have been processed\n",
      "291 0000 data have been processed\n",
      "292 0000 data have been processed\n",
      "293 0000 data have been processed\n",
      "294 0000 data have been processed\n",
      "295 0000 data have been processed\n",
      "296 0000 data have been processed\n",
      "297 0000 data have been processed\n",
      "298 0000 data have been processed\n",
      "299 0000 data have been processed\n",
      "300 0000 data have been processed\n",
      "301 0000 data have been processed\n",
      "302 0000 data have been processed\n",
      "303 0000 data have been processed\n",
      "304 0000 data have been processed\n",
      "305 0000 data have been processed\n",
      "306 0000 data have been processed\n",
      "307 0000 data have been processed\n",
      "308 0000 data have been processed\n",
      "309 0000 data have been processed\n",
      "310 0000 data have been processed\n",
      "311 0000 data have been processed\n",
      "312 0000 data have been processed\n",
      "313 0000 data have been processed\n",
      "314 0000 data have been processed\n",
      "315 0000 data have been processed\n",
      "316 0000 data have been processed\n",
      "317 0000 data have been processed\n",
      "318 0000 data have been processed\n",
      "319 0000 data have been processed\n",
      "320 0000 data have been processed\n",
      "321 0000 data have been processed\n",
      "322 0000 data have been processed\n",
      "323 0000 data have been processed\n",
      "324 0000 data have been processed\n",
      "325 0000 data have been processed\n",
      "326 0000 data have been processed\n",
      "327 0000 data have been processed\n",
      "328 0000 data have been processed\n",
      "329 0000 data have been processed\n",
      "330 0000 data have been processed\n",
      "331 0000 data have been processed\n",
      "332 0000 data have been processed\n",
      "333 0000 data have been processed\n",
      "334 0000 data have been processed\n",
      "335 0000 data have been processed\n",
      "336 0000 data have been processed\n",
      "337 0000 data have been processed\n",
      "338 0000 data have been processed\n",
      "339 0000 data have been processed\n",
      "340 0000 data have been processed\n",
      "341 0000 data have been processed\n",
      "342 0000 data have been processed\n",
      "343 0000 data have been processed\n",
      "344 0000 data have been processed\n",
      "345 0000 data have been processed\n",
      "346 0000 data have been processed\n",
      "347 0000 data have been processed\n",
      "348 0000 data have been processed\n",
      "349 0000 data have been processed\n",
      "350 0000 data have been processed\n",
      "351 0000 data have been processed\n",
      "352 0000 data have been processed\n",
      "353 0000 data have been processed\n",
      "354 0000 data have been processed\n",
      "355 0000 data have been processed\n",
      "356 0000 data have been processed\n",
      "357 0000 data have been processed\n",
      "358 0000 data have been processed\n",
      "359 0000 data have been processed\n",
      "360 0000 data have been processed\n",
      "361 0000 data have been processed\n",
      "362 0000 data have been processed\n",
      "363 0000 data have been processed\n",
      "364 0000 data have been processed\n",
      "365 0000 data have been processed\n",
      "366 0000 data have been processed\n",
      "367 0000 data have been processed\n",
      "368 0000 data have been processed\n",
      "369 0000 data have been processed\n",
      "370 0000 data have been processed\n",
      "371 0000 data have been processed\n",
      "372 0000 data have been processed\n",
      "373 0000 data have been processed\n",
      "374 0000 data have been processed\n",
      "375 0000 data have been processed\n",
      "376 0000 data have been processed\n",
      "377 0000 data have been processed\n",
      "378 0000 data have been processed\n",
      "379 0000 data have been processed\n",
      "380 0000 data have been processed\n",
      "381 0000 data have been processed\n",
      "382 0000 data have been processed\n",
      "383 0000 data have been processed\n",
      "384 0000 data have been processed\n",
      "385 0000 data have been processed\n",
      "386 0000 data have been processed\n",
      "387 0000 data have been processed\n",
      "388 0000 data have been processed\n",
      "389 0000 data have been processed\n",
      "390 0000 data have been processed\n",
      "391 0000 data have been processed\n",
      "392 0000 data have been processed\n",
      "393 0000 data have been processed\n",
      "394 0000 data have been processed\n",
      "395 0000 data have been processed\n",
      "396 0000 data have been processed\n",
      "397 0000 data have been processed\n",
      "398 0000 data have been processed\n",
      "399 0000 data have been processed\n",
      "400 0000 data have been processed\n",
      "401 0000 data have been processed\n",
      "402 0000 data have been processed\n",
      "403 0000 data have been processed\n",
      "404 0000 data have been processed\n",
      "405 0000 data have been processed\n",
      "406 0000 data have been processed\n",
      "407 0000 data have been processed\n",
      "408 0000 data have been processed\n",
      "409 0000 data have been processed\n",
      "410 0000 data have been processed\n",
      "411 0000 data have been processed\n",
      "412 0000 data have been processed\n",
      "413 0000 data have been processed\n",
      "414 0000 data have been processed\n",
      "415 0000 data have been processed\n",
      "416 0000 data have been processed\n",
      "417 0000 data have been processed\n",
      "418 0000 data have been processed\n",
      "419 0000 data have been processed\n",
      "420 0000 data have been processed\n",
      "421 0000 data have been processed\n",
      "422 0000 data have been processed\n",
      "423 0000 data have been processed\n",
      "424 0000 data have been processed\n",
      "425 0000 data have been processed\n",
      "426 0000 data have been processed\n",
      "427 0000 data have been processed\n",
      "428 0000 data have been processed\n",
      "429 0000 data have been processed\n",
      "430 0000 data have been processed\n",
      "431 0000 data have been processed\n",
      "432 0000 data have been processed\n",
      "433 0000 data have been processed\n",
      "434 0000 data have been processed\n",
      "435 0000 data have been processed\n",
      "436 0000 data have been processed\n",
      "437 0000 data have been processed\n",
      "438 0000 data have been processed\n",
      "439 0000 data have been processed\n",
      "440 0000 data have been processed\n",
      "441 0000 data have been processed\n",
      "442 0000 data have been processed\n",
      "443 0000 data have been processed\n",
      "444 0000 data have been processed\n",
      "445 0000 data have been processed\n",
      "446 0000 data have been processed\n",
      "447 0000 data have been processed\n",
      "448 0000 data have been processed\n",
      "449 0000 data have been processed\n",
      "450 0000 data have been processed\n",
      "451 0000 data have been processed\n",
      "452 0000 data have been processed\n",
      "453 0000 data have been processed\n",
      "454 0000 data have been processed\n",
      "455 0000 data have been processed\n",
      "456 0000 data have been processed\n",
      "457 0000 data have been processed\n",
      "458 0000 data have been processed\n",
      "459 0000 data have been processed\n",
      "460 0000 data have been processed\n",
      "461 0000 data have been processed\n",
      "462 0000 data have been processed\n",
      "463 0000 data have been processed\n",
      "464 0000 data have been processed\n",
      "465 0000 data have been processed\n",
      "466 0000 data have been processed\n",
      "467 0000 data have been processed\n",
      "468 0000 data have been processed\n",
      "469 0000 data have been processed\n",
      "470 0000 data have been processed\n",
      "471 0000 data have been processed\n",
      "472 0000 data have been processed\n",
      "473 0000 data have been processed\n",
      "474 0000 data have been processed\n",
      "475 0000 data have been processed\n",
      "476 0000 data have been processed\n",
      "477 0000 data have been processed\n",
      "478 0000 data have been processed\n",
      "479 0000 data have been processed\n",
      "480 0000 data have been processed\n",
      "481 0000 data have been processed\n",
      "482 0000 data have been processed\n",
      "483 0000 data have been processed\n",
      "484 0000 data have been processed\n",
      "485 0000 data have been processed\n",
      "486 0000 data have been processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "487 0000 data have been processed\n",
      "488 0000 data have been processed\n",
      "489 0000 data have been processed\n",
      "490 0000 data have been processed\n",
      "491 0000 data have been processed\n",
      "492 0000 data have been processed\n",
      "493 0000 data have been processed\n",
      "494 0000 data have been processed\n",
      "495 0000 data have been processed\n",
      "496 0000 data have been processed\n",
      "497 0000 data have been processed\n",
      "498 0000 data have been processed\n",
      "499 0000 data have been processed\n",
      "500 0000 data have been processed\n",
      "501 0000 data have been processed\n",
      "502 0000 data have been processed\n",
      "503 0000 data have been processed\n",
      "504 0000 data have been processed\n",
      "505 0000 data have been processed\n",
      "506 0000 data have been processed\n",
      "507 0000 data have been processed\n",
      "508 0000 data have been processed\n",
      "509 0000 data have been processed\n",
      "510 0000 data have been processed\n",
      "511 0000 data have been processed\n",
      "512 0000 data have been processed\n",
      "513 0000 data have been processed\n",
      "514 0000 data have been processed\n",
      "515 0000 data have been processed\n",
      "516 0000 data have been processed\n",
      "517 0000 data have been processed\n",
      "518 0000 data have been processed\n",
      "519 0000 data have been processed\n",
      "520 0000 data have been processed\n",
      "521 0000 data have been processed\n",
      "522 0000 data have been processed\n",
      "523 0000 data have been processed\n",
      "524 0000 data have been processed\n",
      "525 0000 data have been processed\n",
      "526 0000 data have been processed\n",
      "527 0000 data have been processed\n",
      "528 0000 data have been processed\n",
      "529 0000 data have been processed\n",
      "530 0000 data have been processed\n",
      "531 0000 data have been processed\n",
      "532 0000 data have been processed\n",
      "533 0000 data have been processed\n",
      "534 0000 data have been processed\n",
      "535 0000 data have been processed\n",
      "536 0000 data have been processed\n",
      "537 0000 data have been processed\n",
      "538 0000 data have been processed\n",
      "539 0000 data have been processed\n",
      "540 0000 data have been processed\n",
      "541 0000 data have been processed\n",
      "542 0000 data have been processed\n",
      "543 0000 data have been processed\n",
      "544 0000 data have been processed\n",
      "545 0000 data have been processed\n",
      "546 0000 data have been processed\n",
      "547 0000 data have been processed\n",
      "548 0000 data have been processed\n",
      "549 0000 data have been processed\n",
      "550 0000 data have been processed\n",
      "551 0000 data have been processed\n",
      "552 0000 data have been processed\n",
      "553 0000 data have been processed\n",
      "554 0000 data have been processed\n",
      "555 0000 data have been processed\n",
      "556 0000 data have been processed\n",
      "557 0000 data have been processed\n",
      "558 0000 data have been processed\n",
      "559 0000 data have been processed\n",
      "560 0000 data have been processed\n",
      "561 0000 data have been processed\n",
      "562 0000 data have been processed\n",
      "563 0000 data have been processed\n",
      "564 0000 data have been processed\n",
      "565 0000 data have been processed\n",
      "566 0000 data have been processed\n",
      "567 0000 data have been processed\n",
      "568 0000 data have been processed\n",
      "569 0000 data have been processed\n",
      "570 0000 data have been processed\n",
      "571 0000 data have been processed\n",
      "572 0000 data have been processed\n",
      "573 0000 data have been processed\n",
      "574 0000 data have been processed\n",
      "575 0000 data have been processed\n",
      "576 0000 data have been processed\n",
      "577 0000 data have been processed\n",
      "578 0000 data have been processed\n",
      "579 0000 data have been processed\n",
      "580 0000 data have been processed\n",
      "581 0000 data have been processed\n",
      "582 0000 data have been processed\n",
      "583 0000 data have been processed\n",
      "584 0000 data have been processed\n",
      "585 0000 data have been processed\n",
      "586 0000 data have been processed\n",
      "587 0000 data have been processed\n",
      "588 0000 data have been processed\n",
      "589 0000 data have been processed\n",
      "590 0000 data have been processed\n",
      "591 0000 data have been processed\n",
      "592 0000 data have been processed\n",
      "593 0000 data have been processed\n",
      "594 0000 data have been processed\n",
      "595 0000 data have been processed\n",
      "596 0000 data have been processed\n",
      "597 0000 data have been processed\n",
      "598 0000 data have been processed\n",
      "599 0000 data have been processed\n",
      "600 0000 data have been processed\n",
      "601 0000 data have been processed\n",
      "602 0000 data have been processed\n",
      "603 0000 data have been processed\n",
      "604 0000 data have been processed\n",
      "605 0000 data have been processed\n",
      "606 0000 data have been processed\n",
      "607 0000 data have been processed\n",
      "608 0000 data have been processed\n",
      "609 0000 data have been processed\n",
      "610 0000 data have been processed\n",
      "611 0000 data have been processed\n",
      "612 0000 data have been processed\n",
      "613 0000 data have been processed\n",
      "614 0000 data have been processed\n",
      "615 0000 data have been processed\n",
      "616 0000 data have been processed\n",
      "617 0000 data have been processed\n",
      "618 0000 data have been processed\n",
      "619 0000 data have been processed\n",
      "620 0000 data have been processed\n",
      "621 0000 data have been processed\n",
      "622 0000 data have been processed\n",
      "623 0000 data have been processed\n",
      "624 0000 data have been processed\n",
      "625 0000 data have been processed\n",
      "626 0000 data have been processed\n",
      "627 0000 data have been processed\n",
      "628 0000 data have been processed\n",
      "629 0000 data have been processed\n",
      "630 0000 data have been processed\n",
      "631 0000 data have been processed\n",
      "632 0000 data have been processed\n",
      "633 0000 data have been processed\n",
      "634 0000 data have been processed\n",
      "635 0000 data have been processed\n",
      "636 0000 data have been processed\n",
      "637 0000 data have been processed\n",
      "638 0000 data have been processed\n",
      "639 0000 data have been processed\n",
      "640 0000 data have been processed\n",
      "641 0000 data have been processed\n",
      "642 0000 data have been processed\n",
      "643 0000 data have been processed\n",
      "644 0000 data have been processed\n",
      "645 0000 data have been processed\n",
      "646 0000 data have been processed\n",
      "647 0000 data have been processed\n",
      "648 0000 data have been processed\n",
      "649 0000 data have been processed\n",
      "650 0000 data have been processed\n",
      "651 0000 data have been processed\n",
      "652 0000 data have been processed\n",
      "653 0000 data have been processed\n",
      "654 0000 data have been processed\n",
      "655 0000 data have been processed\n",
      "656 0000 data have been processed\n",
      "657 0000 data have been processed\n",
      "658 0000 data have been processed\n",
      "659 0000 data have been processed\n",
      "660 0000 data have been processed\n",
      "661 0000 data have been processed\n",
      "662 0000 data have been processed\n",
      "663 0000 data have been processed\n",
      "664 0000 data have been processed\n",
      "665 0000 data have been processed\n",
      "666 0000 data have been processed\n",
      "667 0000 data have been processed\n",
      "668 0000 data have been processed\n",
      "669 0000 data have been processed\n",
      "670 0000 data have been processed\n",
      "671 0000 data have been processed\n",
      "672 0000 data have been processed\n",
      "673 0000 data have been processed\n",
      "674 0000 data have been processed\n",
      "675 0000 data have been processed\n",
      "676 0000 data have been processed\n",
      "677 0000 data have been processed\n",
      "678 0000 data have been processed\n",
      "679 0000 data have been processed\n",
      "680 0000 data have been processed\n",
      "681 0000 data have been processed\n",
      "682 0000 data have been processed\n",
      "683 0000 data have been processed\n",
      "684 0000 data have been processed\n",
      "685 0000 data have been processed\n",
      "686 0000 data have been processed\n",
      "687 0000 data have been processed\n",
      "688 0000 data have been processed\n",
      "689 0000 data have been processed\n",
      "690 0000 data have been processed\n",
      "691 0000 data have been processed\n",
      "692 0000 data have been processed\n",
      "693 0000 data have been processed\n",
      "694 0000 data have been processed\n",
      "695 0000 data have been processed\n",
      "696 0000 data have been processed\n",
      "697 0000 data have been processed\n",
      "698 0000 data have been processed\n",
      "699 0000 data have been processed\n",
      "700 0000 data have been processed\n",
      "701 0000 data have been processed\n",
      "702 0000 data have been processed\n",
      "703 0000 data have been processed\n",
      "704 0000 data have been processed\n",
      "705 0000 data have been processed\n",
      "706 0000 data have been processed\n",
      "707 0000 data have been processed\n",
      "708 0000 data have been processed\n",
      "709 0000 data have been processed\n",
      "710 0000 data have been processed\n",
      "711 0000 data have been processed\n",
      "712 0000 data have been processed\n",
      "713 0000 data have been processed\n",
      "714 0000 data have been processed\n",
      "715 0000 data have been processed\n",
      "716 0000 data have been processed\n",
      "717 0000 data have been processed\n",
      "718 0000 data have been processed\n",
      "719 0000 data have been processed\n",
      "720 0000 data have been processed\n",
      "721 0000 data have been processed\n",
      "722 0000 data have been processed\n",
      "723 0000 data have been processed\n",
      "724 0000 data have been processed\n",
      "725 0000 data have been processed\n",
      "726 0000 data have been processed\n",
      "727 0000 data have been processed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 0000 data have been processed\n",
      "729 0000 data have been processed\n",
      "730 0000 data have been processed\n",
      "731 0000 data have been processed\n",
      "732 0000 data have been processed\n",
      "733 0000 data have been processed\n",
      "734 0000 data have been processed\n",
      "735 0000 data have been processed\n",
      "736 0000 data have been processed\n",
      "737 0000 data have been processed\n",
      "738 0000 data have been processed\n",
      "739 0000 data have been processed\n",
      "740 0000 data have been processed\n",
      "741 0000 data have been processed\n",
      "742 0000 data have been processed\n",
      "743 0000 data have been processed\n",
      "744 0000 data have been processed\n",
      "745 0000 data have been processed\n",
      "746 0000 data have been processed\n",
      "747 0000 data have been processed\n",
      "748 0000 data have been processed\n",
      "749 0000 data have been processed\n",
      "750 0000 data have been processed\n",
      "751 0000 data have been processed\n",
      "752 0000 data have been processed\n",
      "753 0000 data have been processed\n",
      "754 0000 data have been processed\n",
      "755 0000 data have been processed\n",
      "756 0000 data have been processed\n",
      "757 0000 data have been processed\n",
      "758 0000 data have been processed\n",
      "759 0000 data have been processed\n",
      "760 0000 data have been processed\n",
      "761 0000 data have been processed\n",
      "762 0000 data have been processed\n",
      "763 0000 data have been processed\n",
      "764 0000 data have been processed\n",
      "765 0000 data have been processed\n",
      "766 0000 data have been processed\n",
      "767 0000 data have been processed\n",
      "768 0000 data have been processed\n",
      "769 0000 data have been processed\n",
      "770 0000 data have been processed\n",
      "771 0000 data have been processed\n",
      "772 0000 data have been processed\n",
      "773 0000 data have been processed\n",
      "774 0000 data have been processed\n",
      "775 0000 data have been processed\n",
      "776 0000 data have been processed\n",
      "777 0000 data have been processed\n",
      "778 0000 data have been processed\n",
      "779 0000 data have been processed\n",
      "780 0000 data have been processed\n",
      "781 0000 data have been processed\n",
      "782 0000 data have been processed\n",
      "783 0000 data have been processed\n",
      "784 0000 data have been processed\n",
      "785 0000 data have been processed\n",
      "786 0000 data have been processed\n",
      "787 0000 data have been processed\n",
      "788 0000 data have been processed\n",
      "789 0000 data have been processed\n",
      "790 0000 data have been processed\n",
      "791 0000 data have been processed\n",
      "792 0000 data have been processed\n",
      "793 0000 data have been processed\n",
      "794 0000 data have been processed\n",
      "795 0000 data have been processed\n",
      "796 0000 data have been processed\n",
      "797 0000 data have been processed\n",
      "798 0000 data have been processed\n",
      "799 0000 data have been processed\n",
      "800 0000 data have been processed\n",
      "801 0000 data have been processed\n",
      "802 0000 data have been processed\n",
      "803 0000 data have been processed\n",
      "804 0000 data have been processed\n",
      "805 0000 data have been processed\n",
      "806 0000 data have been processed\n",
      "807 0000 data have been processed\n",
      "808 0000 data have been processed\n",
      "809 0000 data have been processed\n",
      "810 0000 data have been processed\n",
      "811 0000 data have been processed\n",
      "812 0000 data have been processed\n",
      "813 0000 data have been processed\n",
      "814 0000 data have been processed\n",
      "815 0000 data have been processed\n",
      "816 0000 data have been processed\n",
      "817 0000 data have been processed\n",
      "818 0000 data have been processed\n",
      "819 0000 data have been processed\n",
      "820 0000 data have been processed\n",
      "821 0000 data have been processed\n",
      "822 0000 data have been processed\n",
      "823 0000 data have been processed\n",
      "824 0000 data have been processed\n",
      "825 0000 data have been processed\n",
      "826 0000 data have been processed\n",
      "827 0000 data have been processed\n",
      "828 0000 data have been processed\n",
      "829 0000 data have been processed\n",
      "830 0000 data have been processed\n",
      "831 0000 data have been processed\n",
      "832 0000 data have been processed\n",
      "833 0000 data have been processed\n",
      "834 0000 data have been processed\n",
      "CPU times: user 9h 33min 28s, sys: 4min 27s, total: 9h 37min 56s\n",
      "Wall time: 9h 39min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lst = []\n",
    "i = 1\n",
    "for index,row in airbnb_reviews.iterrows():\n",
    "    if (index/10000).is_integer():\n",
    "        print(i, '0000 data have been processed')\n",
    "        i = i +1\n",
    "        \n",
    "    try:\n",
    "        if(detect(airbnb_reviews.iloc[index]['comments']) == \"en\"):\n",
    "            lst.append(index)\n",
    "    except:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row selection\n",
    "airbnb_reviews_en = airbnb_reviews.iloc[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the csv to feather\n",
    "airbnb_reviews_en.reset_index().to_feather('data/airbnb_reviews_en.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join the dataset based on same listing id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_reviews_en = airbnb_reviews_en.loc[:, ['listing_id','comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6819301</td>\n",
       "      <td>3102110</td>\n",
       "      <td>James and Yuna's place was awesome. They where...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6819302</td>\n",
       "      <td>13744</td>\n",
       "      <td>A nice appartment in a friendly and quiet neig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         listing_id                                           comments\n",
       "6819301     3102110  James and Yuna's place was awesome. They where...\n",
       "6819302       13744  A nice appartment in a friendly and quiet neig..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_reviews_en.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# concatenate the string \n",
    "airbnb_reviews_en['comments'] = airbnb_reviews_en.groupby(['listing_id'])['comments'].transform(lambda x : ' '.join(x)) \n",
    "airbnb_reviews_en = airbnb_reviews_en.drop_duplicates()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>488835</td>\n",
       "      <td>First time using Airbnb and couldn't be happie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>549036</td>\n",
       "      <td>I had really good time in Alex apartment and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13546118</td>\n",
       "      <td>Place is pretty nice and Sergio was a big help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2643611</td>\n",
       "      <td>Patrica was the perfect host. She meet us at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2263681</td>\n",
       "      <td>Manuel is a really great host and the apt is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  listing_id                                           comments\n",
       "0      0      488835  First time using Airbnb and couldn't be happie...\n",
       "1      1      549036  I had really good time in Alex apartment and I...\n",
       "2      2    13546118  Place is pretty nice and Sergio was a big help...\n",
       "3      4     2643611  Patrica was the perfect host. She meet us at t...\n",
       "4      7     2263681  Manuel is a really great host and the apt is a..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_reviews_en.reset_index().to_feather('data/airbnb_reviews_en_join.feather')\n",
    "pd.read_feather('data/airbnb_reviews_en_join.feather').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "airbnb_reviews_en_join = pd.read_feather('data/airbnb_reviews_en_join.feather').drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>488835</td>\n",
       "      <td>First time using Airbnb and couldn't be happie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>549036</td>\n",
       "      <td>I had really good time in Alex apartment and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13546118</td>\n",
       "      <td>Place is pretty nice and Sergio was a big help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2643611</td>\n",
       "      <td>Patrica was the perfect host. She meet us at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2263681</td>\n",
       "      <td>Manuel is a really great host and the apt is a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id                                           comments\n",
       "0      488835  First time using Airbnb and couldn't be happie...\n",
       "1      549036  I had really good time in Alex apartment and I...\n",
       "2    13546118  Place is pretty nice and Sergio was a big help...\n",
       "3     2643611  Patrica was the perfect host. She meet us at t...\n",
       "4     2263681  Manuel is a really great host and the apt is a..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_reviews_en_join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc 2 Vec for comments\n",
    "#### The code below is from COMP30027 Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file with only English comments\n",
    "airbnb_reviews_en = pd.read_feather('data/airbnb_reviews_en.feather').drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file with only English comments\n",
    "airbnb_reviews_en_join = pd.read_feather('data/airbnb_reviews_en_join.feather').drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = airbnb_reviews_en_join['comments']\n",
    "test = airbnb_reviews_en_join['comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 9s, sys: 3.99 s, total: 7min 13s\n",
      "Wall time: 7min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# preprocess text and compute counts\n",
    "vocab = CountVectorizer(stop_words='english').fit(train_corpus)\n",
    "\n",
    "# generate counts for a new set of documents\n",
    "doc_emb = vocab.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the comment to 50 dimensional vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 4 µs, total: 9 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# size of the output vector\n",
    "vec_size = 50\n",
    "\n",
    "# function to preprocess and tokenize text\n",
    "def tokenize_corpus(txt, tokens_only=False):\n",
    "    for i, line in enumerate(txt):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 32s, sys: 3min 7s, total: 11min 39s\n",
      "Wall time: 12min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenize a training corpus\n",
    "corpus = list(tokenize_corpus(train_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 ms, sys: 10.4 ms, total: 11.8 ms\n",
      "Wall time: 19.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train doc2vec on the training corpus\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 58s, sys: 37.2 s, total: 3min 35s\n",
      "Wall time: 4min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 39min 23s, sys: 8min 21s, total: 1h 47min 44s\n",
      "Wall time: 48min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 12s, sys: 2min 34s, total: 10min 46s\n",
      "Wall time: 14min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# tokenize new documents\n",
    "doc = list(tokenize_corpus(test, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate embeddings for the new documents\n",
    "doc_emb = np.zeros((len(doc),vec_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "CPU times: user 1h 9min 59s, sys: 4min 44s, total: 1h 14min 43s\n",
      "Wall time: 1h 20min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(doc)):\n",
    "    if (i/10000).is_integer():\n",
    "        print(i)\n",
    "    doc_emb[i,:] = model.infer_vector(doc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.08 s, sys: 7.51 s, total: 8.59 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = pd.DataFrame(doc_emb)\n",
    "data.columns = data.columns.astype(str)\n",
    "new_df = pd.concat([airbnb_reviews_en_join, data], axis = 1)\n",
    "#new_df.reset_index().to_feather('data/airbnb_reviews_en_join_doc2vec50.feather')\n",
    "#pd.read_feather('data/airbnb_reviews_en_join_doc2vec50.feather').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>listing_id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>488835</td>\n",
       "      <td>-0.172850</td>\n",
       "      <td>0.054249</td>\n",
       "      <td>1.331955</td>\n",
       "      <td>0.624849</td>\n",
       "      <td>-0.289155</td>\n",
       "      <td>0.352968</td>\n",
       "      <td>-0.168172</td>\n",
       "      <td>-0.026205</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.313929</td>\n",
       "      <td>0.960765</td>\n",
       "      <td>-0.409866</td>\n",
       "      <td>0.665558</td>\n",
       "      <td>-1.174518</td>\n",
       "      <td>0.394543</td>\n",
       "      <td>-0.461035</td>\n",
       "      <td>0.561439</td>\n",
       "      <td>0.387918</td>\n",
       "      <td>-0.116501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>549036</td>\n",
       "      <td>-1.000408</td>\n",
       "      <td>-1.492268</td>\n",
       "      <td>1.580612</td>\n",
       "      <td>1.604159</td>\n",
       "      <td>0.080560</td>\n",
       "      <td>2.180467</td>\n",
       "      <td>-0.874364</td>\n",
       "      <td>0.136403</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.852686</td>\n",
       "      <td>-0.180197</td>\n",
       "      <td>-0.088430</td>\n",
       "      <td>0.152311</td>\n",
       "      <td>-2.935486</td>\n",
       "      <td>1.093438</td>\n",
       "      <td>1.591154</td>\n",
       "      <td>0.095329</td>\n",
       "      <td>1.272898</td>\n",
       "      <td>-0.357847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13546118</td>\n",
       "      <td>-1.340827</td>\n",
       "      <td>-0.015176</td>\n",
       "      <td>1.288301</td>\n",
       "      <td>1.518229</td>\n",
       "      <td>-0.263480</td>\n",
       "      <td>1.270910</td>\n",
       "      <td>-0.058099</td>\n",
       "      <td>0.376789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091462</td>\n",
       "      <td>1.028206</td>\n",
       "      <td>-0.863812</td>\n",
       "      <td>-0.259800</td>\n",
       "      <td>-2.160436</td>\n",
       "      <td>0.234394</td>\n",
       "      <td>1.207856</td>\n",
       "      <td>0.688972</td>\n",
       "      <td>1.361456</td>\n",
       "      <td>-0.100542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2643611</td>\n",
       "      <td>-0.641008</td>\n",
       "      <td>-0.350837</td>\n",
       "      <td>-0.236163</td>\n",
       "      <td>1.827108</td>\n",
       "      <td>0.733946</td>\n",
       "      <td>0.815207</td>\n",
       "      <td>-0.349609</td>\n",
       "      <td>0.291637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483328</td>\n",
       "      <td>1.034008</td>\n",
       "      <td>-0.312193</td>\n",
       "      <td>1.007967</td>\n",
       "      <td>-0.990584</td>\n",
       "      <td>0.524995</td>\n",
       "      <td>-0.357787</td>\n",
       "      <td>-0.171655</td>\n",
       "      <td>0.297890</td>\n",
       "      <td>-1.260579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2263681</td>\n",
       "      <td>-2.314118</td>\n",
       "      <td>-0.464722</td>\n",
       "      <td>0.678646</td>\n",
       "      <td>1.288942</td>\n",
       "      <td>0.647862</td>\n",
       "      <td>4.017908</td>\n",
       "      <td>-0.183250</td>\n",
       "      <td>-0.242754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193845</td>\n",
       "      <td>-0.032328</td>\n",
       "      <td>-1.314451</td>\n",
       "      <td>1.484645</td>\n",
       "      <td>-1.331208</td>\n",
       "      <td>0.995373</td>\n",
       "      <td>0.405309</td>\n",
       "      <td>-1.083297</td>\n",
       "      <td>1.158279</td>\n",
       "      <td>-1.147393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  listing_id         0         1         2         3         4  \\\n",
       "0      0      488835 -0.172850  0.054249  1.331955  0.624849 -0.289155   \n",
       "1      1      549036 -1.000408 -1.492268  1.580612  1.604159  0.080560   \n",
       "2      2    13546118 -1.340827 -0.015176  1.288301  1.518229 -0.263480   \n",
       "3      3     2643611 -0.641008 -0.350837 -0.236163  1.827108  0.733946   \n",
       "4      4     2263681 -2.314118 -0.464722  0.678646  1.288942  0.647862   \n",
       "\n",
       "          5         6         7  ...        40        41        42        43  \\\n",
       "0  0.352968 -0.168172 -0.026205  ... -0.313929  0.960765 -0.409866  0.665558   \n",
       "1  2.180467 -0.874364  0.136403  ... -0.852686 -0.180197 -0.088430  0.152311   \n",
       "2  1.270910 -0.058099  0.376789  ...  0.091462  1.028206 -0.863812 -0.259800   \n",
       "3  0.815207 -0.349609  0.291637  ...  0.483328  1.034008 -0.312193  1.007967   \n",
       "4  4.017908 -0.183250 -0.242754  ...  0.193845 -0.032328 -1.314451  1.484645   \n",
       "\n",
       "         44        45        46        47        48        49  \n",
       "0 -1.174518  0.394543 -0.461035  0.561439  0.387918 -0.116501  \n",
       "1 -2.935486  1.093438  1.591154  0.095329  1.272898 -0.357847  \n",
       "2 -2.160436  0.234394  1.207856  0.688972  1.361456 -0.100542  \n",
       "3 -0.990584  0.524995 -0.357787 -0.171655  0.297890 -1.260579  \n",
       "4 -1.331208  0.995373  0.405309 -1.083297  1.158279 -1.147393  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the feature 'comment' and save to feather again (not necessary)\n",
    "new_df2 = new_df.loc[:,new_df.columns!='comments']\n",
    "new_df2.reset_index().to_feather('data/airbnb_reviews_en_join_doc2vec50.feather')\n",
    "pd.read_feather('data/airbnb_reviews_en_join_doc2vec50.feather').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>488835</td>\n",
       "      <td>First time using Airbnb and couldn't be happie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>549036</td>\n",
       "      <td>I had really good time in Alex apartment and I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13546118</td>\n",
       "      <td>Place is pretty nice and Sergio was a big help...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2643611</td>\n",
       "      <td>Patrica was the perfect host. She meet us at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2263681</td>\n",
       "      <td>Manuel is a really great host and the apt is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359868</td>\n",
       "      <td>16453105</td>\n",
       "      <td>Comfortable little studio! Perfect for going t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359869</td>\n",
       "      <td>17814136</td>\n",
       "      <td>A great place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359870</td>\n",
       "      <td>9679914</td>\n",
       "      <td>Ken and Pinky are amazing hosts! They really h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359871</td>\n",
       "      <td>16477143</td>\n",
       "      <td>Even though we didn't see Enchanta her roommat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359872</td>\n",
       "      <td>16634931</td>\n",
       "      <td>Eric was friendly and very nice and his apartm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>359873 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        listing_id                                           comments\n",
       "0           488835  First time using Airbnb and couldn't be happie...\n",
       "1           549036  I had really good time in Alex apartment and I...\n",
       "2         13546118  Place is pretty nice and Sergio was a big help...\n",
       "3          2643611  Patrica was the perfect host. She meet us at t...\n",
       "4          2263681  Manuel is a really great host and the apt is a...\n",
       "...            ...                                                ...\n",
       "359868    16453105  Comfortable little studio! Perfect for going t...\n",
       "359869    17814136                                      A great place\n",
       "359870     9679914  Ken and Pinky are amazing hosts! They really h...\n",
       "359871    16477143  Even though we didn't see Enchanta her roommat...\n",
       "359872    16634931  Eric was friendly and very nice and his apartm...\n",
       "\n",
       "[359873 rows x 2 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airbnb_reviews_en_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
